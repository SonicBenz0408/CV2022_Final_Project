# -*- coding: utf-8 -*-
"""cv_final

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B5Um2MTxehhD1kAz_7pVuRrKZLsCLOs2
"""

!nvidia-smi

# 10w
!gdown --id 1hhcsXxGehgf_wf2QJKSuwB7e3xxrTYn9 --output "data.zip"

!unzip -q 'data.zip'

"""###Import"""

!pip install pickle5

import os
import random
import torch.nn as nn
import torch
from torchvision import transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from torchvision.transforms.functional import rotate, hflip, pad, crop, affine
from sklearn.cluster import KMeans

"""###dataset"""

from torch.utils.data import Dataset
# import pickle
import pickle5 as pickle
from PIL import Image
from torchvision.transforms.functional import hflip, pad, crop
import matplotlib.pyplot as plt

class Img_Dataset_train(Dataset):
    def __init__(self, data_path, transforms, have_anno=True):
        
        if have_anno:
            with open(os.path.join(data_path, "annot.pkl"), "rb") as f:
                annot = pickle.load(f)
                self.names, self.feats= annot
        else:
            self.names = os.listdir(data_path)
            self.feats = [[0.]] * len(self.names) 

        self.imgs = []
        for name in self.names:
            img = os.path.join(data_path, name)
            self.imgs.append(img)

        self.transforms = transforms

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, index):
        img = Image.open(self.imgs[index])
        feat = torch.FloatTensor(self.feats[index])

        # # flip
        # hflip_n = np.random.choice([True, False], size=1, p=[0.5, 0.5])

        # if hflip_n:
        #     img = hflip(img)
        #     feat[:, 0] = 383 - feat[:, 0]
        #     feat = torch.cat((torch.flip(feat[0:17, :], [0]), torch.flip(feat[17:27, :], [0]), feat[27:31, :], torch.flip(feat[31:36, :], [0]), torch.flip(feat[42:46, :], [0]), torch.flip(feat[46:48, :], [0]), torch.flip(feat[36:40, :], [0]), torch.flip(feat[40:42, :], [0]), torch.flip(feat[48:55, :], [0]), torch.flip(feat[55:60, :], [0]), torch.flip(feat[60:65, :], [0]), torch.flip(feat[65:68, :], [0])))
        
        new_img = self.transforms(img)        
        img.close()

        return new_img, feat

class Img_Dataset(Dataset):
    def __init__(self, data_path, transforms, have_anno=True):
        
        if have_anno:
            with open(os.path.join(data_path, "annot.pkl"), "rb") as f:
                annot = pickle.load(f)
                self.names, self.feats= annot
        else:
            self.names = os.listdir(data_path)
            self.feats = [[0.]] * len(self.names) 

        self.imgs = []
        for name in self.names:
            img = os.path.join(data_path, name)
            self.imgs.append(img)

        self.transforms = transforms

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, index):
        img = Image.open(self.imgs[index])
        new_img = self.transforms(img)
        feat = torch.FloatTensor(self.feats[index])
        img.close()
        
        return new_img, feat

"""###pyramid"""

from re import M
import torch.nn as nn
import torch

class InvertedResidual(nn.Module):
    def __init__(self, inp, oup, stride, expand_ratio):
        super(InvertedResidual, self).__init__()
        self.stride = stride
        assert stride in [1, 2]

        hidden_dim = int(inp * expand_ratio)
        self.use_res_connect = self.stride == 1 and inp == oup

        if expand_ratio == 1:
            self.conv = nn.Sequential(
                # dw
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
                # pw-linear
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
                nn.InstanceNorm2d(oup, affine=True)
            )
        else:
            self.conv = nn.Sequential(
                # pw
                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
                # dw
                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),
                nn.BatchNorm2d(hidden_dim),
                nn.ReLU6(inplace=True),
                # pw-linear
                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),
                nn.BatchNorm2d(oup),
                nn.InstanceNorm2d(oup, affine=True)
            )

    def forward(self, x):
        if self.use_res_connect:
            return x + self.conv(x)
        else:
            return self.conv(x)

class PyramidNet(nn.Module):

    def __init__(self):
        super(PyramidNet, self).__init__()

        self.dim = 32

        # expand, out_channels, layer_num, stride
        #self.mobile_config = [
        #    [1, 24, 1, 1],
        #    [2, 48, 1, 2],
        #    [2, 48, 5, 2],
        #    [2, 96, 1, 2],
        #    [4, 192, 6, 2],
        #    [2, 24, 1, 1]
        #]
        self.mobile_config = [
            [1, 16, 1, 1],
            [2, 32, 1, 2],
            [2, 32, 5, 2],
            [2, 64, 1, 2],
            [4, 128, 6, 2],
            [2, 16, 1, 1]
        ]
        self.input_conv = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=self.dim, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(self.dim),
            nn.ReLU6(inplace=True)
        )

        self.feature_extractor = [self.input_conv]
        
        input_channel = self.dim

        for t, c, n, s in self.mobile_config:
            output_channel = c
            for i in range(n):
                if i == 0:
                    self.feature_extractor.append(InvertedResidual(input_channel, output_channel, s, expand_ratio=t))
                else:
                    self.feature_extractor.append(InvertedResidual(input_channel, output_channel, 1, expand_ratio=t))
                input_channel = output_channel

        self.feature_extractor = nn.Sequential(*self.feature_extractor)

        # (N, 32, 12, 12)
        self.down1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)
        
        # (N, 64, 6, 6)
        self.down2 = nn.Sequential(
            nn.BatchNorm2d(32),
            nn.ReLU6(inplace=True),
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=6, stride=1)
        )

        # (N, 128, 1, 1)
        # 16 * 12 * 12 + 32 * 6 * 6 + 128
        self.fc = nn.Sequential(
            nn.Linear(16 * 12 * 12 + 32 * 6 * 6 + 128, 2 * 68),
        )

    def forward(self, input):
        s1 = self.feature_extractor(input)
        s2 = self.down1(s1)
        s3 = self.down2(s2)

        s1 = torch.flatten(s1, 1)
        s2 = torch.flatten(s2, 1)
        s3 = torch.flatten(s3, 1)

        feature = torch.concat((s1, s2, s3), dim=1)
        output = self.fc(feature)
        output = torch.reshape(output, (output.shape[0], 68, 2))

        return output


class AnchorPyramidNet(nn.Module):

    def __init__(self, anchor_num=24):
        super(AnchorPyramidNet, self).__init__()

        self.dim = 32

        # expand, out_channels, layer_num, stride
        #self.mobile_config = [
        #    [1, 24, 1, 1],
        #    [2, 48, 1, 2],
        #    [2, 48, 5, 2],
        #    [2, 96, 1, 2],
        #    [4, 192, 6, 2],
        #    [2, 24, 1, 1]
        #]
        self.mobile_config = [
            [1, 16, 1, 1],
            [2, 32, 1, 2],
            [2, 32, 5, 2],
            [2, 64, 1, 2],
            [4, 128, 6, 2],
            [2, 16, 1, 1]
        ]
        self.input_conv = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=self.dim, kernel_size=3, stride=2, padding=1, bias=False),
            nn.BatchNorm2d(self.dim),
            nn.ReLU6(inplace=True)
        )

        self.feature_extractor = [self.input_conv]
        
        input_channel = self.dim

        for t, c, n, s in self.mobile_config:
            output_channel = c
            for i in range(n):
                if i == 0:
                    self.feature_extractor.append(InvertedResidual(input_channel, output_channel, s, expand_ratio=t))
                else:
                    self.feature_extractor.append(InvertedResidual(input_channel, output_channel, 1, expand_ratio=t))
                input_channel = output_channel

        self.feature_extractor = nn.Sequential(*self.feature_extractor)

        

        # (N, 16, 12, 12)
        self.down1 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=1)
        
        # (N, 32, 6, 6)
        self.down2 = nn.Sequential(
            nn.BatchNorm2d(32),
            nn.ReLU6(inplace=True),
            nn.Conv2d(in_channels=32, out_channels=128, kernel_size=6, stride=1)
        )

        # (N, 128, 1, 1)
        # 16 * 12 * 12 + 32 * 6 * 6 + 128
        self.fc = nn.Sequential(
            nn.Linear(16 * 12 * 12 + 32 * 6 * 6 + 128, 2 * 68),
        )

        self.regression = nn.Sequential(
            nn.Linear(16 * 12 * 12 + 32 * 6 * 6 + 128, 4* 24 * 2 * 68)
        )

        self.confidence = nn.Sequential(
            nn.Linear(16 * 12 * 12 + 32 * 6 * 6 + 128, 4* 24),
            nn.Softmax(dim=1)
        )

    def forward(self, input):
        s1 = self.feature_extractor(input)
        s2 = self.down1(s1)
        s3 = self.down2(s2)

        s1 = torch.flatten(s1, 1)
        s2 = torch.flatten(s2, 1)
        s3 = torch.flatten(s3, 1)

        feature = torch.concat((s1, s2, s3), dim=1)
        regression_output = self.regression(feature)
        confidence_output = self.confidence(feature)
        
        regression_output = torch.reshape(regression_output, (regression_output.shape[0], 4*24, 68, 2))
        return regression_output, confidence_output

"""###loss"""

from turtle import forward
import numpy as np
import torch.nn as nn
import torch

class NMELoss(nn.Module):
    def __init__(self):
        super(NMELoss, self).__init__()

    def forward(self, x, y):
        dis = x - y
        dis = torch.sqrt(torch.sum(torch.pow(dis, 2), 2))
        dis = torch.mean(dis, 1)
        dis = torch.sum(dis) / 384
        dis = dis / x.shape[0]

        return dis


class WingLoss(nn.Module):
    def __init__(self, gamma=10, eps=2):
        super(WingLoss, self).__init__()
        self.gamma = gamma
        self.eps = eps

    def forward(self, x, y):

        # (N, 68, 2)
        dis = x - y
        dis = torch.sqrt(torch.sum(torch.pow(dis, 2), 2))

        small = torch.sum(self.gamma * torch.log(1 + dis[dis < self.gamma]/self.eps))
        large = torch.sum(dis[dis >= self.gamma] - (self.gamma - self.gamma * np.log(1 + self.gamma/self.eps)))

        loss = (small + large) / x.shape[0]

        return loss

class WeightedL2Loss(nn.Module):
    def __init__(self, weights):
        super(WeightedL2Loss, self).__init__()

        self.weights = weights
    
    def forward(self, x, y):

        dis = x - y
        dis = torch.sqrt(torch.sum(torch.pow(dis, 2), 2))
        dis = self.weights.repeat(dis.shape[0], 1).cuda() * dis
        dis = torch.mean(dis, 1)
        dis = torch.sum(dis)

        return dis

# right face: 1~9
# left face: 10~17
# right eyebrow: 18~22
# left eyebrow: 23~27
# nose: 28~31 32~36
# right eye: 37~42
# left eye: 43~48
# outer mouth: 49~60
# inner mouth: 61~68

class CenterLoss(nn.Module):
    def __init__(self):
        super(CenterLoss, self).__init__()
    
    def forward(self, x, y):
        # (N, 68, 2)
        face = torch.sum((torch.mean(x[:, 0:9, 0], 1) - torch.mean(y[:, 0:9, 0], 1)) ** 2 + (torch.mean(x[:, 0:9, 1], 1) - torch.mean(y[:, 0:9, 1], 1)) ** 2) + torch.sum((torch.mean(x[:, 9:17, 0], 1) - torch.mean(y[:, 9:17, 0], 1)) ** 2 + (torch.mean(x[:, 9:17, 1], 1) - torch.mean(y[:, 9:17, 1], 1)) ** 2)
        eyebrow = torch.sum((torch.mean(x[:, 17:22, 0], 1) - torch.mean(y[:, 17:22, 0], 1)) ** 2 + (torch.mean(x[:, 17:22, 1], 1) - torch.mean(y[:, 17:22, 1], 1)) ** 2) + torch.sum((torch.mean(x[:, 22:27, 0], 1) - torch.mean(y[:, 22:27, 0], 1)) ** 2 + (torch.mean(x[:, 22:27, 1], 1) - torch.mean(y[:, 22:27, 1], 1)) ** 2)
        nose = torch.sum((torch.mean(x[:, 27:36, 0], 1) - torch.mean(y[:, 27:36, 0], 1)) ** 2 + (torch.mean(x[:, 27:36, 1], 1) - torch.mean(y[:, 27:36, 1], 1)) ** 2)
        eye = torch.sum((torch.mean(x[:, 36:42, 0], 1) - torch.mean(y[:, 36:42, 0], 1)) ** 2 + (torch.mean(x[:, 36:42, 1], 1) - torch.mean(y[:, 36:42, 1], 1)) ** 2) + torch.sum((torch.mean(x[:, 42:48, 0], 1) - torch.mean(y[:, 42:48, 0], 1)) ** 2 + (torch.mean(x[:, 42:48, 1], 1) - torch.mean(y[:, 42:48, 1], 1)) ** 2)
        mouth = torch.sum((torch.mean(x[:, 48:60, 0], 1) - torch.mean(y[:, 48:60, 0], 1)) ** 2 + (torch.mean(x[:, 48:60, 1], 1) - torch.mean(y[:, 48:60, 1], 1)) ** 2) + torch.sum((torch.mean(x[:, 60:68, 0], 1) - torch.mean(y[:, 60:68, 0], 1)) ** 2 + (torch.mean(x[:, 60:68, 1], 1) - torch.mean(y[:, 60:68, 1], 1)) ** 2)
        
        dis = ( face + eyebrow + nose + eye + mouth ) / x.shape[0]
        return dis


class RegressionLoss(nn.Module):
    def __init__(self, anchors):
        super(RegressionLoss, self).__init__()

        self.anchors = anchors
    
    def forward(self, x, y, c):
        anc = self.anchors.repeat(x.shape[0], 1, 1, 1).cuda()
        
        # (N, A, 68, 2)
        offset_pred = x - anc
        y = y.unsqueeze(dim=1).repeat_interleave(self.anchors.shape[0], dim=1)
        offset_gt =  y - anc

        loss = torch.sum(c * torch.sum(torch.abs(offset_pred - offset_gt), dim=(2, 3))) / x.shape[0]

        return loss

class ConfidenceLoss(nn.Module):
    def __init__(self, anchors, beta=0.1):
        super(ConfidenceLoss, self).__init__()

        self.beta = beta
        self.anchors = anchors
    
    def forward(self, y, c):
        anc = self.anchors.repeat(y.shape[0], 1, 1, 1).cuda()
        
        y = y.unsqueeze(dim=1).repeat_interleave(self.anchors.shape[0], dim=1)
        c_target =  torch.tanh( (self.beta * 2 * 68) / (torch.sum((torch.sum(( y - anc ) ** 2, dim=3) ** 0.5), dim=2)))

        loss = torch.sum(-c * torch.log(c_target) - (1 - c) * torch.log(1 - c_target)) / y.shape[0]

        return loss

"""###rotation"""

import math
import torch

def rotate_coord(coord, angle):
    angle = float(angle)
    rad = torch.tensor(-angle * math.pi / 180)
    c = torch.cos(rad)
    s = torch.sin(rad)
    rotation_matrix = torch.stack([torch.stack([c, -s]),
                                    torch.stack([s, c])])
    
    shifting = torch.full(coord.shape, -383/2)
    coord = coord + shifting
    new_coord = coord @ rotation_matrix.t()
    new_coord = new_coord - shifting
    return new_coord

"""###preprocess"""

# fix random seeds for reproducibility
SEED = 7414
torch.manual_seed(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
random.seed(SEED)
np.random.seed(SEED)

train_path = "./data/synthetics_train"
val_path = "./data/aflw_val"

mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]

train_tfms = transforms.Compose([
    # transforms.ColorJitter(brightness=0.2, saturation=0.2, hue=0.2),
    #transforms.GaussianBlur(5),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

val_tfms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

train_set = Img_Dataset_train(train_path, train_tfms)
val_set = Img_Dataset(val_path, val_tfms)

print("Dataset complete!")

# Anchor Generation
k = 3
all_feats = np.reshape(np.array(train_set.feats), (len(train_set.feats), 136))
kmeans_tool = KMeans(n_clusters=k).fit(all_feats)
clusters = torch.FloatTensor(np.reshape(kmeans_tool.cluster_centers_, (k, 68, 2)))

#for i in range(k):
#    plt.figure(i, figsize=(4, 4))
#    plt.xlim(0, 383)
#    plt.ylim(383, 0)
#    plt.scatter(clusters[i, :, 0], clusters[i, :, 1], c="r", s=2)
#plt.show()

# anchors = []
# for i in range(k):
#     for j in range(8):
#         anchors.append(torch.unsqueeze(rotate_coord(clusters[i], 45 * j), 0))

# anchors = torch.cat(anchors)
# Hyperparameters
batch_size = 32
learning_rate = 0.001
# max_epoch = 40
max_epoch = 1
noise_epoch = 1
crop_epoch = 4
noise_size = 41
shifting = noise_size // 2
pad_size = 90

center_gamma = 0.05
conf_gamma = 0.5
c_th = 0.6

weights = [1.] * 27 + [20] * 9 + [1] * 24 + [20] * 8
weights = torch.FloatTensor(weights)
#print(weights)

#black = torch.FloatTensor([[[-2.1179]], [[-2.0357]], [[-1.8044]]]).repeat(1, noise_size, noise_size)
white = torch.FloatTensor([[[2.2489]], [[2.4286]], [[2.6400]]]).repeat(1, noise_size, noise_size)
#white = torch.Tensor([2.2489, 2.4286, 2.6400]).repeat(384, 384, 1)
save_path = "./log/MobileNetv2_32_anchor"

os.makedirs(save_path, exist_ok=True)
# Data loader
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_set, batch_size=1, shuffle=False)

print("Dataloader complete!")

anchors = []
# shift = [
#          [-50, -50], [0, -50], [50, -50],
#          [-50, 0 ], [0, 0 ], [50, 0 ],
#          [-50, 50], [0, 50], [50, 50]
# ]
shift = [
         [-25, -25], [25, -25],
         [-25, 25], [25, 25 ]
]
for s in shift:
  anc = (torch.stack((clusters[:, :, 0]+s[0], clusters[:, :, 1]+s[1]), dim=2))
  for i in range(k):
    for j in range(8):
        anchors.append(torch.unsqueeze(rotate_coord(anc[i], 45 * j), 0))
anchors = torch.cat(anchors)
print(anchors.size())

i = 20
plt.figure(i, figsize=(4, 4))
plt.xlim(0, 383)
plt.ylim(383, 0)
plt.scatter(anchors[i, :, 0], anchors[i, :, 1], c="r", s=2)
plt.show()

# for i in range(len(anchors)):
#    plt.figure(i, figsize=(4, 4))
#    plt.xlim(0, 383)
#    plt.ylim(383, 0)
#    plt.scatter(anchors[i, :, 0], anchors[i, :, 1], c="r", s=2)
#    print(anchors[i])
# plt.show()
# i = 21
# plt.figure(i, figsize=(4, 4))
# plt.xlim(0, 383)
# plt.ylim(383, 0)
# plt.scatter(anchors[i, :, 0], anchors[i, :, 1], c="r", s=2)
# print(anchors[i])
# plt.show()

"""###Training"""

# Preparation
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#model = PyramidNet().to(device)
model = AnchorPyramidNet().to(device)
optimizer = torch.optim.Adam([{"params":model.parameters(), "initial_lr": learning_rate}], lr=learning_rate)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, len(train_loader)*10, 0.5)

#criterion = WingLoss()
#centerLoss = CenterLoss()
#criterion = WeightedL2Loss(weights=weights)
#criterion = NMELoss()

regLoss = RegressionLoss(anchors)
confLoss = ConfidenceLoss(anchors)
evaluation = NMELoss()

best_NME = 100.
train_loss_curve, val_loss_curve = [], []
NME_curve = []
print("Training Start!")
for epoch in range(max_epoch):

    model.train()

    wing_loss, center_loss = 0., 0.
    train_loss, val_loss = 0., 0.
    for image, coords in tqdm(train_loader):
        #angle_list = np.random.randn(len(image)) * 40
        
        if epoch > crop_epoch:
            crop_list = np.random.choice([True, False], size=len(image), p=[0.1, 0.9])
            for i in range(len(image)):
                #plt.figure(0)
                #plt.imshow(np.transpose(image[i], (1, 2, 0)))
                #plt.scatter(coords[i, :, 0], coords[i, :, 1], c="g", s=2)
                
                # crop
                if crop_list[i]:
                    h_shift = int(np.ceil(np.random.random() * (pad_size)))
                    v_shift = int(np.ceil(np.random.random() * (pad_size)))
                    coords[i, :, 0] += h_shift
                    coords[i, :, 1] += v_shift
                    image[i] = affine(image[i], translate=(h_shift, v_shift))

        angle_list = 180 * (2 * np.random.rand(len(image)) - 1)
        # for i in range(len(image)):
        #     #plt.figure(0)
        #     #plt.imshow(np.transpose(image[i], (1, 2, 0)))
        #     #plt.scatter(coords[i, :, 0], coords[i, :, 1], c="g", s=2)

        #     image[i] = rotate(image[i], angle_list[i])
        #     coords[i] = rotate_coord(coords[i], angle_list[i])

        #     #plt.figure(1)
        #     #plt.imshow(np.transpose(image[i], (1, 2, 0)))
        #     #plt.scatter(coords[i, :, 0], coords[i, :, 1], c="g", s=2)
        #     #plt.show()

        if epoch > noise_epoch:
            random_idxs = np.random.randint(0, 68, size=image.shape[0])
            for i in range(image.shape[0]):
                x, y = coords[i][random_idxs[i]]
                x, y = int(x.floor()), int(y.floor())
                shift_x_low = max(shifting - x, 0)
                shift_x_high = max(shifting - (383 - x), 0)
                shift_y_low = max(shifting - y, 0)
                shift_y_high = max(shifting - (383 - y), 0)

                image[i][:, y-shifting+shift_y_low-shift_y_high:y+shifting+1-shift_y_high+shift_y_low, x-shifting+shift_x_low-shift_x_high:x+shifting+1-shift_x_high+shift_x_low] = white

        image, coords = image.to(device), coords.to(device)
        
        # (N, A, 68, 2), (N, A)
        r_output, c_output = model(image)
        # print(c_output.size())
        loss = regLoss(r_output, coords, c_output) + conf_gamma * confLoss(coords, c_output)
        

        """
        output = model(image)

        wing_l = criterion(output, coords)
        center_l = center_gamma * centerLoss(output, coords)
        loss =  wing_l + center_l
        
        train_loss += loss.item()
        wing_loss += wing_l
        center_loss += center_l
        """
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        scheduler.step()
    model.eval()
    NME_loss = 0.
    # for image, coords in tqdm(val_loader):
        
    #     image, coords = image.to(device), coords.to(device)

    #     with torch.no_grad():
            
    #         r_output, c_output = model(image)

    #         loss = regLoss(r_output, coords, c_output) + conf_gamma * confLoss(coords, c_output)
            
    #         c_output[c_output < c_th] = 0.
            
    #         output = torch.sum(c_output * r_output, dim=1)
    #         """
    #         output = model(image)

    #         loss = criterion(output, coords) + center_gamma * centerLoss(output, coords)
    #         """
    #         val_loss += loss.item()

    #         NME = evaluation(output, coords)
    #         NME_loss += NME.item()
    
    
    # train_loss /= len(train_loader)
    # #wing_loss /= len(train_loader)
    # #center_loss /= len(train_loader)
    # val_loss /= len(val_loader)
    # NME_loss /= len(val_loader)

    

    # log_content = f'Epoch: {epoch+1}/{max_epoch}, Training loss: {train_loss:.4f}, Validation loss: {val_loss:.4f}, NME: {NME_loss*100:.3f}%'
    # print(log_content)
    # print(f'Wing loss: {wing_loss:.4f}, Center loss: {center_loss:.4f}')
    # if NME_loss < best_NME:
    #     best_NME = NME_loss
    #     torch.save(model.state_dict(), os.path.join(save_path, "last_model.pth"))
    #     print("save best model.")

    # with open(os.path.join(save_path,  "log.txt"), "a") as log_file:
    #     log_file.write(log_content)

    # train_loss_curve.append(train_loss)
    # val_loss_curve.append(val_loss)
    # NME_curve.append(NME_loss)


x_axis = np.arange(max_epoch)
plt.figure(0)
plt.plot(x_axis, train_loss_curve, c="r")
plt.plot(x_axis, val_loss_curve, c="b")
plt.title("Training Curve")
plt.legend(["train loss", "val loss"])

plt.figure(1)
plt.plot(x_axis, NME_curve, c="r")
plt.title("NME Loss Curve")
plt.show()